{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coderdivine/UniqueWordDataset/blob/main/BankDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qGbylkGZxsy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def extract_fields_from_xlsx(filename):\n",
        "    # Read all sheets of the Excel file into a dictionary of pandas DataFrames\n",
        "    all_sheets = pd.read_excel(filename, sheet_name=None)\n",
        "\n",
        "    # Initialize variables to store extracted fields\n",
        "    reference = []\n",
        "    remarks = []\n",
        "    transaction_details = []\n",
        "    ref_no = []\n",
        "    Details = []\n",
        "\n",
        "    for sheet_name, df in all_sheets.items():\n",
        "        # Iterate over the columns in the DataFrame\n",
        "        print(f\"*************** First Files {sheet_name} ***************\")\n",
        "        for column in df.columns:\n",
        "            # Remove leading/trailing spaces and convert column name to lowercase\n",
        "            cleaned_column = column.strip().lower()\n",
        "\n",
        "            # Check if the cleaned column header matches the desired fields\n",
        "            if 'reference' in cleaned_column:\n",
        "                reference.extend(df[column].tolist())\n",
        "            elif 'remarks' in cleaned_column:\n",
        "                remarks.extend(df[column].tolist())\n",
        "            elif 'transaction details' in cleaned_column:\n",
        "                transaction_details.extend(df[column].tolist())\n",
        "            elif 'ref no' in cleaned_column:\n",
        "                ref_no.extend(df[column].tolist())\n",
        "            elif 'details' in cleaned_column:\n",
        "                Details.extend(df[column].tolist())\n",
        "\n",
        "\n",
        "    # Return the extracted fields as a dictionary\n",
        "    extracted_fields = {\n",
        "        'Reference': reference,\n",
        "        'Remarks': remarks,\n",
        "        'Transaction Details': transaction_details,\n",
        "        'Ref No': ref_no,\n",
        "        'Details':Details\n",
        "    }\n",
        "\n",
        "    return extracted_fields\n",
        "\n",
        "# Specify the path to the folder containing the XLSX files\n",
        "folder_path = '/content/BankDatasets'\n",
        "\n",
        "# Iterate over the files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Check if the file is an XLSX file\n",
        "    if filename.endswith('.xlsx'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Call the function to extract the fields\n",
        "        extracted_data = extract_fields_from_xlsx(file_path)\n",
        "        #print(extracted_data)\n",
        "\n",
        "        # Access the extracted fields\n",
        "        reference_list = extracted_data['Reference']\n",
        "        remarks_list = extracted_data['Remarks']\n",
        "        transaction_details_list = extracted_data['Transaction Details']\n",
        "        ref_no_list = extracted_data['Ref No']\n",
        "        details = extracted_data[\"Details\"]\n",
        "        print(extracted_data)\n",
        "\n",
        "        # Process the extracted fields as needed\n",
        "        # ...\n",
        "\n",
        "        # Print or perform any other operations\n",
        "        print(f\"Processed file: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JajK8kycjIwg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Assuming you have the extracted fields stored in separate lists\n",
        "transaction_details_list = extracted_data['Transaction Details']\n",
        "reference_list = extracted_data['Reference']\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({'sentence': transaction_details_list, 'reference': reference_list})\n",
        "#df = pd.DataFrame({'transaction_details': transaction_details_list, 'reference': reference_list})\n",
        "\n",
        "# Remove np.nan values and replace them with empty strings in transaction_details and reference columns\n",
        "df['sentence'].fillna('', inplace=True)\n",
        "df['reference'].fillna('', inplace=True)\n",
        "\n",
        "# Initialize an empty list to store the unique patterns\n",
        "unique_patterns = []\n",
        "\n",
        "# Iterate over the DataFrame rows\n",
        "for index, row in df.iterrows():\n",
        "    reference = row['reference']\n",
        "    transaction_details = row['sentence']\n",
        "\n",
        "    # Check if the previous and current transaction_details are different\n",
        "    if index > 0 and transaction_details != df.loc[index - 1, 'sentence']:\n",
        "        # Check the type of the transaction_details\n",
        "        if isinstance(df.loc[index - 1, 'sentence'], str) and isinstance(transaction_details, str):\n",
        "            # Find unique patterns between the previous and current transaction_details\n",
        "            unique_words = list(set(df.loc[index - 1, 'sentence'].split()) - set(transaction_details.split()))\n",
        "            if not unique_words:\n",
        "                unique_words = [\"\"]  # Set empty list to [\"\"] if no unique words found\n",
        "            unique_patterns.append({'sentence': transaction_details, 'unique_words': unique_words})\n",
        "        else:\n",
        "            # Handle non-string transaction_details by returning an empty string\n",
        "            unique_patterns.append({'sentence': transaction_details, 'unique_words': [\"\"]})\n",
        "    else:\n",
        "        # Handle the first row where there is no previous transaction_details to compare with\n",
        "        if isinstance(transaction_details, str):\n",
        "            unique_words = list(set(transaction_details.split()))\n",
        "            if not unique_words:\n",
        "                unique_words = [\"\"]  # Set empty list to [\"\"] if no unique words found\n",
        "            unique_patterns.append({'sentence': transaction_details, 'unique_words': unique_words})\n",
        "\n",
        "# Convert the list of unique patterns to JSON format\n",
        "new_df_json = json.dumps(unique_patterns)\n",
        "new_df = pd.DataFrame(unique_patterns)\n",
        "\n",
        "# Print the JSON data\n",
        "print(new_df_json)\n",
        "print(new_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoVbfylQoQ4v"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-multilearn transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "745de8d10b4e4cc6b2fef5390e239d43",
            "ebc2221829f141d0b8a61375310371c1",
            "a4004d6044194d77947e1829efb088ed",
            "59e1181f877746158ad6790deb102e71"
          ]
        },
        "id": "5tDt7cjBoNXv",
        "outputId": "55202f15-f3c9-4d5c-913c-c2a00815b075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0                                                      []\n",
            "1                                                      []\n",
            "2        [30-04-22, MONEY, TO, LEVY-9, TXNS:, ELEC, TRSF]\n",
            "3       [AUTOTOPUP, NIGERIA, FIP:FTS/TRF/MTN, SETTLEMENT]\n",
            "4                         [NELLOBYTE, SME002, /TAS220287]\n",
            "                              ...                        \n",
            "1064                          [UYO/TAS229742/21284/GOLDE]\n",
            "1065    [from, RULE, GOLDEN, COMM, UYO/TAS229762/21284...\n",
            "1066      [ICMS/5768310522192020:TAS229764/21283/GOLDENR]\n",
            "1067    [ICMS/5733290522145507:TAS227700/19323/GOLDENR...\n",
            "1068                           [TRANSFER, BALANCE, SWEEP]\n",
            "Name: unique_words, Length: 1069, dtype: object\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['/TAS222655', '/TAS226599', '18-05-22', '24-05-22', '26-05-22', '30-04-22', 'BL/TAS228832', 'C45527/19609/OPTIMUM', 'C45642/19742/TSHABRON', 'C45713/19741/TSHABRON', 'FCSL:C46217/19461/WOLID', 'FCSL:C46222/19461/WOLID', 'FCSL:TAS221000/19941/EASY', 'FCSL:TAS221887/19941/EASY', 'FCSL:TAS222250/19941/EASY', 'FCSL:TAS222291/19941/EASY', 'FCSL:TAS223042/19941/EASY', 'FCSL:TAS223411/19941/EASY', 'FCSL:TAS223413/19941/EASY', 'FCSL:TAS224596/19941/EASY', 'FCSL:TAS224612/19941/EASY', 'FCSL:TAS225241/19941/EASY', 'FCSL:TAS225244/19941/EASY', 'FCSL:TAS225467/19941/EASY', 'FCSL:TAS226272/19941/EASY', 'FCSL:TAS226406/19386/SEAMAN', 'FCSL:TAS226772/19941/EASY', 'FCSL:TAS226774/19941/EASY', 'FCSL:TAS227197/19386/SEAMAN', 'FCSL:TAS227293/19409/STRATFORD', 'FCSL:TAS228280/19941/EASY', 'FCSL:TAS228302/19941/EASY', 'FCSL:TAS228687/19386/SEAMAN', 'FCSL:TAS229223/19941/EASY', 'FCSL:c45114/19461/WOLID', 'FCSL:c45144/19464/WOLID', 'FCSL:c45363/19461/WOLID', 'FCSL:c45488/19464/WOLID', 'FCSL:c45833/19461/WOLID', 'FIP:CMB/TRF/0451017579073!MTN', 'ICMS/5712130522174807:TAS224518/21283/GOLDENR', 'ICMS/5712270522103512:TAS227938/19803/AWAKE', 'ICMS/5714160522115958:19965/TAS223990/RINGO', 'ICMS/5714190522110711:TAS225963/19803/AWAKE', 'ICMS/5715110522120503:TAS223573/19323/GOLDENR', 'ICMS/5716160522163144:TAS223852/19323/GOLDENR', 'ICMS/5716200522171746:TAS226692/21283/GOLDENR', 'ICMS/5716240522105101:TAS226065/19805/AWAKE', 'ICMS/5716290522144613:TAS228259/21283/GOLDENR', 'ICMS/5717040522102658:TAS221172/19805/AWAKE', 'ICMS/5717130522144502:RINGO', 'ICMS/5719170522171028:RINGO', 'ICMS/5720120522141242:TAS223875/19803/AWAKE', 'ICMS/5720300522153526:TAS227168/21284/GOLDENR', 'ICMS/5724270522163548:19965/TAS226529/RINGO', 'ICMS/5725250522110037:19965/TAS227683/RINGO', 'ICMS/5725270522130159:TAS228499/19409/STRATFO', 'ICMS/5725270522183106:TAS227510/19323/GOLDENR', 'ICMS/5726040522133850:TAS221044/21283/GOLDENR', 'ICMS/5728100522173115:TAS223357/19803/AWAKE', 'ICMS/5728300522201541:TAS229340/19965/RINGO', 'ICMS/5729100522132250:TAS223145/19803/AWAKE', 'ICMS/5729230522121658:TAS226219/21283/GOLDENR', 'ICMS/5729270522130334:TAS228512/19409/STRATFO', 'ICMS/5730100522123426:TAS223197/21283/GOLDENR', 'ICMS/5730260522164027:TAS228204/21283/GOLDENR', 'ICMS/5732040522134120:TAS221045/21283/GOLDENR', 'ICMS/5732110522122835:TAS223554/19805/AWAKE', 'ICMS/5733300522103518:19965/TAS228183/RINGO', 'ICMS/5734100522194506:TAS223400/21283/GOLDENR', 'ICMS/5734300522173302:TAS229274/21283/GOLDENR', 'ICMS/5736170522123357:TAS225173/19803/AWAKE', 'ICMS/5738040522134138:TAS220232/21284/GOLDENR', 'ICMS/5738100522124116:TAS223085/19805/AWAKE', 'ICMS/5740200522102405:TAS226378/19805/AWAKE', 'ICMS/5741100522163612:TAS222687/19323/GOLDENR', 'ICMS/5743060522162005:TAS221755/21283/GOLDENR', 'ICMS/5747180522162447:19965/TAS224799/RINGO', 'ICMS/5750040522134214:TAS220609/21284/GOLDENR', 'ICMS/5751230522104416:19965/TAS225771/RINGO', 'ICMS/5752120522100125:TAS223814/19803/AWAKE', 'ICMS/5752260522183459:RINGO', 'ICMS/5754190522135252:TAS226122/19323/GOLDENR', 'ICMS/5758100522132554:TAS223146/19803/AWAKE', 'ICMS/5758250522093327:19967/TAS225766/RINGO', 'ICMS/5759160522130838:TAS222997/19803/AWAKE', 'ICMS/5760110522120629:TAS223505/19803/AWAKE', 'ICMS/5760120522110650:TAS223822/19805/AWAKE', 'ICMS/5760190522160805:TAS226172/19805/AWAKE', 'ICMS/5760270522102921:TAS227091/19803/AWAKE', 'ICMS/5764110522122139:TAS223538/19805/AWAKE', 'ICMS/5764200522171537:TAS226688/21283/GOLDENR', 'ICMS/5765100522163846:TAS222551/21283/GOLDENR', 'ICMS/5766110522165530:TAS223649/19805/AWAKE', 'ICMS/5768250522132122:TAS227148/19323/GOLDENR', 'ICMS/5770180522160523:TAS225747/19805/AWAKE', 'ICMS/5772040522104119:TAS221137/19803/AWAKE', 'ICMS/5772230522164856:TAS225107/21284/GOLDENR', 'ICMS/5773040522103352:TAS221421/19805/AWAKE', 'ICMS/5774050522101059:19965/TAS221608/RINGO', 'ICMS/5774300522102949:TAS228880/19805/AWAKE', 'ICMS/5775100522173633:TAS223379/19965/RINGO', 'ICMS/5776130522195638:TAS224578/19323/GOLDENR', 'ICMS/5776200522161929:TAS226548/19803/AWAKE', 'ICMS/5780110522164630:TAS223694/19803/AWAKE', 'ICMS/5780200522101254:TAS226377/19803/AWAKE', 'ICMS/5780200522191735:TAS226723/21284/GOLDENR', 'ICMS/5781170522133521:19965/TAS225355/RINGO', 'ICMS/5784240522104843:TAS226106/19805/AWAKE', 'ICMS/5785120522111019:TAS223850/19805/AWAKE', 'ICMS/5785180522172313:TAS225132/21283/GOLDENR', 'ICMS/5786090522102357:TAS221136/19805/AWAKE', 'ICMS/5787110522154526:19965/TAS221548/RINGO', 'ICMS/5788060522160829:TAS221754/19323/GOLDENR', 'ICMS/5788170522125210:TAS225156/19805/AWAKE', 'ICMS/5788200522101834:TAS226380/19803/AWAKE', 'ICMS/5789190522105552:TAS225412/19803/AWAKE', 'ICMS/5789250522132303:TAS227178/21283/GOLDENR', 'ICMS/5790100522133036:TAS223186/19803/AWAKE', 'ICMS/5790130522184100:TAS224557/19965/RINGO', 'ICMS/5791160522092221:TAS224762/21283/GOLDENR', 'ICMS/5791170522174220:TAS225512/19965/RINGO', 'ICMS/5792040522173409:TAS221685/21283/GOLDENR', 'ICMS/5793110522074144:TAS223473/19803/AWAKE', 'ICMS/5793120522110442:TAS223349/19805/AWAKE', 'ICMS/5793270522102615:TAS227071/19803/AWAKE', 'ICMS/5793300522160007:TAS227167/21284/GOLDENR', 'ICMS/5794160522125605:TAS224029/19803/AWAKE', 'ICMS/5795180522162844:19965/TAS223640/RINGO', 'ICMS/5796040522112145:TAS221442/19409/STRATFO', 'ICMS/5796200522150737:TAS225455/21283/GOLDENR', 'LEVY-30', 'LEVY-35', 'LEVY-43', 'LEVY-9', 'LIMITED/TAS225027/19770/HI', 'LIMITED/TAS225033/19771/HI', 'NELLO', 'PH2/TAS221667/21283/GOLDE', 'PH2/TAS224019/21283/GOLDE', 'RESOUR/TAS224898/197', 'RESOURCES', 'RVSL/TAS225615/19650/STRATFORD', 'TAS219615/20045/DEMMY', 'TAS219774/20045/DEMMY', 'TAS220091/20044/DEMMY', 'TAS221580/20044/DEMMY', 'TAS221612/20044/DEMMY', 'TAS221706/20045/DEMMY', 'TAS222033/19672/KSCP', 'TAS222335/20045/DEMMY', 'TAS222340/20045/DEMMY', 'TAS222710/20045/DEMMY', 'TAS222907/20044/DEMMY', 'TAS222931/19386/SEAMAN', 'TAS222999/20044/DEMMY', 'TAS223040/20044/DEMMY', 'TAS223077/20044/DEMMY', 'TAS223167/19386/SEAMAN', 'TAS223498/19409/STRATFORD', 'TAS223499/19409/STRATFORD', 'TAS224071/20045/DEMMY', 'TAS224091/20044/DEMMY', 'TAS224376', 'TAS224383/20044/DEMMY', 'TAS224839/19713/COCKPIT', 'TAS224871/19409/STRATFORD', 'TAS225105/20044/DEMMY', 'TAS225123/20044/DEMMY', 'TAS225127/20045/DEMMY', 'TAS225257/19941/EASY', 'TAS225329/19409/STRATFORD', 'TAS225369/20044/DEMY', 'TAS225416/20045/DEMMY', 'TAS225435/20045/DEMMY', 'TAS225452', 'TAS226257/20044/DEMMY', 'TAS226427/19409/STRATFORD', 'TAS226443/20044/DEMMY', 'TAS227003/20045/DEMMY', 'TAS227038/20045/DEMMY', 'TAS227039/20045/DEMMY', 'TAS227109/20044/DEMMY', 'TAS227127/19650/INTERSWITCH', 'TAS227495/20044/DEMMY', 'TAS227498/20044/DEMMY', 'TAS228158/65032/WOLID', 'TAS228298/20044/DEMMY', 'TAS228313', 'TAS228618/20044/DEMMY', 'TAS228632/20044/DEMMY', 'TAS229446/20044/DEMMY', 'TECH', 'USMAN/C45211', 'USMAN/C45212', 'USMAN/C46187', 'USMAN/C47312', 'USMAN/C47769', 'UYO/TAS223329/21284/GOLDE', 'UYO/TAS224532/21284/GOLDE', 'UYO/TAS226596/21284/GOLDE', 'UYO/TAS226685/21284/GOLDN', 'UYO/TAS227505/21284/GOLDE', 'UYO/TAS227890/21284/GOLDE', 'UYO/TAS229763/21284/GOLDE', 'VENTURES/TAS221179/19386/SEAMAN/ANA', 'VENTURES/TAS226182/19386/SEAMAN/ANA', 'WORLDS/TAS225004/19952'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "745de8d10b4e4cc6b2fef5390e239d43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebc2221829f141d0b8a61375310371c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4004d6044194d77947e1829efb088ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59e1181f877746158ad6790deb102e71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Average Loss: 0.5652\n",
            "Epoch 2 - Average Loss: 0.3077\n",
            "Epoch 3 - Average Loss: 0.1750\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "print(new_df['unique_words'])\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    new_df['sentence'], new_df['unique_words'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Transforming text into numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Converting the target variable to a binary indicator matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_binary = mlb.fit_transform(y_train)\n",
        "y_test_binary = mlb.transform(y_test)\n",
        "\n",
        "# Fine-tuning BERT on our specific task\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length, labels):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        text = item['sentence']\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(\n",
        "            text=text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'][0],  # Access the tokenized input_ids\n",
        "            'attention_mask': inputs['attention_mask'][0],  # Access the attention mask\n",
        "            'labels': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(df.iloc[X_train.index], tokenizer, max_length=128, labels=y_train_binary)\n",
        "test_dataset = CustomDataset(df.iloc[X_test.index], tokenizer, max_length=128, labels=y_test_binary)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(mlb.classes_)).to(device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop for fine-tuning BERT\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = []\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        y_pred_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "\n",
        "    y_pred_probs = np.array(y_pred_probs)\n",
        "    y_pred_binary = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculating loss (log loss)\n",
        "loss = log_loss(y_test_binary, y_pred_probs)\n",
        "print(\"Loss:\", loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzmKibFmoVGN"
      },
      "outputs": [],
      "source": [
        "# Example sentence for prediction\n",
        "new_sentence = \"FIP:FTS/TRF/MTN NIGERIA AUTOTOPUP SETTLEMENT\"\n",
        "\n",
        "# Tokenize the new sentence\n",
        "inputs = tokenizer(\n",
        "    text=new_sentence,\n",
        "    max_length=128,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Move inputs to the device\n",
        "input_ids = inputs['input_ids'].to(device)\n",
        "attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "# Make prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predicted_probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "# Convert probabilities to binary predictions (using threshold 0.5)\n",
        "predicted_binary = (predicted_probs >= 0.5).astype(int)\n",
        "\n",
        "# Inverse transform the binary predictions to obtain the predicted unique words\n",
        "predicted_unique_words = mlb.inverse_transform(predicted_binary)\n",
        "\n",
        "print(\"Predicted unique words:\", predicted_unique_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSb5oSvfx94XLZGg/ODL22",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}